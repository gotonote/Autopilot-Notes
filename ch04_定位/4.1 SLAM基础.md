# 4.1 SLAM基础

## 一、SLAM概述

### 1. 什么是SLAM

SLAM（Simultaneous Localization and Mapping），即**同步定位与地图构建**，是自动驾驶、机器人领域的核心技术之一。它要解决的核心问题是：一个机器人在未知环境中运动时，如何同时估计自身的位姿（位置和姿态）并构建周围环境的地图。

用一句话概括：SLAM = **定位（Localization）** + **建图（Mapping）**。

数学上，SLAM可以形式化为一个状态估计问题。设机器人在时刻$t$的状态为$\mathbf{x}_t$，包括位置$\mathbf{p}_t$和姿态$\mathbf{q}_t$（或旋转矩阵$\mathbf{R}_t$），观测为$\mathbf{z}_t$，控制输入为$\mathbf{u}_t$。SLAM的目标是估计整个轨迹$\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n\}$和地图$\mathbf{M} = \{\mathbf{m}_1, \mathbf{m}_2, ..., \mathbf{m}_m\}$的后验概率：

$$P(\mathbf{X}, \mathbf{M} | \mathbf{Z}, \mathbf{U})$$

其中$\mathbf{Z} = \{\mathbf{z}_1, \mathbf{z}_2, ..., \mathbf{z}_n\}$为所有观测，$\mathbf{U} = \{\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n\}$为所有控制输入。

<!-- 插入一行空格 -->
<div align=center>
<img src="../imgs/slam_problem.png" width="500" height="300">
</div>
<div align=center>图1. SLAM问题示意图：机器人同时估计自身轨迹和构建环境地图</div>
<!-- 插入一行空格 -->

### 2. SLAM的核心挑战

SLAM问题之所以困难，主要源于以下几个核心挑战：

**（1）鸡生蛋问题**
准确的定位需要精确的地图，而精确的地图又需要准确的定位。两者相互依赖，形成"先有鸡还是先有蛋"的困境。

**（2）噪声与不确定性**
传感器测量存在噪声，运动模型有误差，这些因素都会引入不确定性。SLAM算法需要能够处理这些不确定性并进行鲁棒估计。

**（3）计算复杂度**
随着机器人运动时间的增加，地图规模会不断扩大，状态变量的维度也会持续增长，导致计算复杂度急剧上升。

**（4）数据关联问题**
当环境中存在相似特征时，如何正确地将当前观测与地图中的特征进行匹配，是SLAM中极具挑战性的问题。错误的关联会导致地图扭曲甚至算法发散。

**（5）回环检测与校正**
当机器人回到曾经访问过的地方时，需要检测到这个"回环"（Loop Closure）并校正累积误差。这是保证地图全局一致性的关键。

### 3. SLAM问题的数学建模

SLAM问题通常使用**概率图模型**来描述。最常用的方法是**扩展卡尔曼滤波（Extended Kalman Filter, EKF）**和**因子图优化（Factor Graph Optimization）**。

#### 3.1 状态空间模型

SLAM的状态通常包含机器人位姿和路标点位置：

$$\mathbf{x} = [\mathbf{x}_r^T, \mathbf{m}_1^T, \mathbf{m}_2^T, ..., \mathbf{m}_n^T]^T$$

其中机器人位姿$\mathbf{x}_r = [x, y, z, \phi, \theta, \psi]^T$（3D情况下），包含位置和姿态角。

#### 3.2 运动模型

机器人的运动可以用状态转移方程描述：

$$\mathbf{x}_t = f(\mathbf{x}_{t-1}, \mathbf{u}_t) + \mathbf{w}_t$$

其中$f(\cdot)$为非线性运动函数，$\mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q}_t)$为过程噪声。

对于简单的差分驱动机器人，运动模型可以表示为：

$$
\begin{bmatrix} x_t \\ y_t \\ \theta_t \end{bmatrix} = 
\begin{bmatrix} x_{t-1} + v \Delta t \cos(\theta_{t-1}) \\ y_{t-1} + v \Delta t \sin(\theta_{t-1}) \\ \theta_{t-1} + \omega \Delta t \end{bmatrix} + \mathbf{w}_t
$$

其中$v$为线速度，$\omega$为角速度。

#### 3.3 观测模型

观测模型描述了传感器如何测量环境特征：

$$\mathbf{z}_t = h(\mathbf{x}_t) + \mathbf{v}_t$$

其中$h(\cdot)$为观测函数，$\mathbf{v}_t \sim \mathcal{N}(0, \mathbf{R}_t)$为测量噪声。

## 二、视觉SLAM与激光SLAM

根据使用的主要传感器不同，SLAM可以分为**视觉SLAM**（Visual SLAM）和**激光SLAM**（LiDAR SLAM）两大类。

<!-- 插入一行空格 -->
<div align=center>
<img src="../imgs/slam_comparison.png" width="600" height="350">
</div>
<div align=center>图2. 视觉SLAM与激光SLAM对比</div>
<!-- 插入一行空格 -->

### 1. 视觉SLAM

#### 1.1 基本原理

视觉SLAM使用摄像头作为主要传感器，通过分析图像序列来估计相机运动和场景结构。其核心流程包括：

**（1）特征提取与匹配**
从图像中提取稳定的特征点（如SIFT、SURF、ORB等），并在相邻帧之间进行匹配。

**（2）运动估计**
根据特征匹配结果，计算相邻帧之间的相机运动。常用的方法包括：
- **2D-2D**：对极几何（Epipolar Geometry）
- **2D-3D**：Perspective-n-Point (PnP)
- **3D-3D**：迭代最近点（ICP）

**（3）局部优化**
通过Bundle Adjustment（光束法平差）优化相机位姿和三维点位置：

$$\min_{\mathbf{R}, \mathbf{t}, \mathbf{X}} \sum_{i,j} \rho \left( \left\| \mathbf{u}_{ij} - \pi(\mathbf{R}_i \mathbf{X}_j + \mathbf{t}_i) \right\|^2 \right)$$

其中$\pi(\cdot)$为投影函数，$\rho(\cdot)$为鲁棒核函数。

**（4）回环检测**
检测是否回到曾经访问过的地方，触发全局优化以校正累积误差。

#### 1.2 单目、双目与RGB-D视觉SLAM

| 类型 | 优点 | 缺点 | 代表算法 |
|------|------|------|----------|
| **单目** | 成本低、结构简单 | 尺度不确定性、需要初始化 | ORB-SLAM、LSD-SLAM、DSO |
| **双目** | 可恢复绝对尺度 | 计算量大、基线限制精度 | ORB-SLAM2、S-PTAM |
| **RGB-D** | 直接获取深度 | 受光照影响、范围有限 | RTAB-Map、ElasticFusion |

#### 1.3 直接法与特征点法

**特征点法**（Indirect Method）：
- 提取图像特征点并进行描述子匹配
- 优点：对光照变化鲁棒、地图可重用
- 缺点：特征提取耗时、对纹理要求较高
- 代表：ORB-SLAM系列

**直接法**（Direct Method）：
- 直接使用像素灰度信息，最小化光度误差
- 优点：速度快、可利用更多信息
- 缺点：对光照敏感、需要较好的初始估计
- 代表：LSD-SLAM、DSO

**半直接法**（Semi-Direct Method）：
- 结合两者优点，如SVO（Semi-Direct Visual Odometry）

### 2. 激光SLAM

#### 2.1 基本原理

激光SLAM使用激光雷达（LiDAR）作为主要传感器，直接测量环境的距离信息。其核心流程包括：

**（1）点云配准**
将当前帧的点云与历史点云或局部地图进行配准，计算位姿变换。最常用的是**ICP（Iterative Closest Point）**算法及其变体。

ICP的目标是最小化点云间的距离：

$$E(\mathbf{R}, \mathbf{t}) = \sum_{i=1}^{N} \left\| \mathbf{p}_i - (\mathbf{R} \mathbf{q}_i + \mathbf{t}) \right\|^2$$

其中$\mathbf{p}_i$和$\mathbf{q}_i$为对应点。

**（2）栅格地图构建**
将点云转换为占据栅格地图（Occupancy Grid Map），每个格子表示该区域被占据的概率：

$$P(m_i | z_{1:t}, x_{1:t})$$

**（3）回环检测**
通过扫描匹配或基于描述子的方法检测回环。

#### 2.2 2D与3D激光SLAM

**2D激光SLAM**：
- 使用单线激光雷达，适用于平面环境
- 计算效率高，适合低速场景
- 代表：Gmapping、Cartographer（2D模式）

**3D激光SLAM**：
- 使用多线激光雷达（如Velodyne、Ouster等）
- 可处理复杂三维环境
- 数据量大，对计算资源要求高
- 代表：LOAM、LeGO-LOAM、LIO-SAM

### 3. 视觉SLAM vs 激光SLAM对比

| 对比维度 | 视觉SLAM | 激光SLAM |
|----------|----------|----------|
| **传感器成本** | 低（摄像头） | 高（激光雷达） |
| **计算资源** | 中等 | 较高（3D点云） |
| **光照依赖** | 强 | 无 |
| **纹理依赖** | 强 | 无 |
| **深度精度** | 相对低 | 高 |
| **地图类型** | 稀疏/半稠密/稠密 | 点云/栅格 |
| **回环检测** | 容易（外观信息丰富） | 较难（几何信息单一） |
| **适用场景** | 室内、室外（白天） | 室内外均可、全天候 |

## 三、经典SLAM算法详解

### 1. ORB-SLAM系列

ORB-SLAM是视觉SLAM领域最具影响力的开源算法之一，由Raul Mur-Artal等人开发。

#### 1.1 ORB-SLAM（单目）

**核心创新**：
- 基于ORB特征，计算效率高且对旋转鲁棒
- 三线程并行架构：Tracking、Local Mapping、Loop Closing
- 基于词袋模型（BoW）的快速回环检测

**系统架构**：

<!-- 插入一行空格 -->
<div align=center>
<img src="../imgs/orb_slam_arch.png" width="550" height="400">
</div>
<div align=center>图3. ORB-SLAM系统架构</div>
<!-- 插入一行空格 -->

**主要流程**：
1. **Tracking线程**：实时跟踪相机位姿，确定何时插入新关键帧
2. **Local Mapping线程**：处理新关键帧，进行局部BA优化
3. **Loop Closing线程**：检测回环，计算相似变换，执行位姿图优化

#### 1.2 ORB-SLAM2

在单目版本基础上增加了：
- **双目相机支持**：利用立体视觉恢复绝对尺度
- **RGB-D相机支持**：直接获取深度信息
- 改进的回环检测和地图复用机制

#### 1.3 ORB-SLAM3

最新版本增加了：
- **视觉惯性SLAM（VI-SLAM）**：融合IMU信息
- **多地图系统（Atlas）**：支持跟踪丢失时的地图保存与合并
- **最大后验估计（MAP）**：替代传统的关键帧选择

### 2. LOAM系列（激光SLAM）

LOAM（Lidar Odometry and Mapping）是3D激光SLAM的经典算法，由Ji Zhang和Sanjiv Singh提出。

#### 2.1 LOAM核心思想

将SLAM问题分解为两个独立的算法并行运行：

**高频低精度的里程计（Odometry）**：
- 频率：10Hz
- 使用点到线和点到面的配准
- 优化当前帧到局部地图的位姿

**低频高精度的建图（Mapping）**：
- 频率：1Hz
- 使用所有点云进行精细配准
- 维护全局地图

**特征提取**：
LOAM从点云中提取两种特征：
- **边缘特征**（Edge Features）：位于尖锐边缘的点
- **平面特征**（Planar Features）：位于平坦表面的点

特征选择基于局部曲率$c$：

$$c = \frac{1}{|S| \cdot \|\mathbf{p}_i\|} \left\| \sum_{j \in S, j \neq i} (\mathbf{p}_j - \mathbf{p}_i) \right\|$$

其中$S$为邻域点集，曲率大的为边缘特征，曲率小的为平面特征。

#### 2.2 LeGO-LOAM

LeGO-LOAM（Lightweight and Ground-Optimized LOAM）针对地面车辆优化：
- **地面点分割**：分离地面点，减少计算量
- **两步LM优化**：先优化$z, roll, pitch$，再优化$x, y, yaw$
- **回环检测**：基于ICP的位姿图优化

#### 2.3 LIO-SAM

LIO-SAM（Lidar Inertial Odometry via Smoothing and Mapping）融合了激光雷达和IMU：
- **因子图框架**：使用iSAM2进行增量优化
- **紧耦合IMU**：IMU预积分约束
- **多传感器融合**：可融合GPS、高度计等
- **关键帧选择**：基于位姿变化自适应选择

因子图包含四种约束：
1. **IMU预积分因子**：连接连续关键帧
2. **激光里程计因子**：帧间配准约束
3. **GPS因子**：全局位置约束（可选）
4. **回环因子**：检测回环后的位姿约束

<!-- 插入一行空格 -->
<div align=center>
<img src="../imgs/lio_sam_graph.png" width="500" height="350">
</div>
<div align=center>图4. LIO-SAM因子图结构</div>
<!-- 插入一行空格 -->

### 3. 其他重要算法

#### 3.1 VINS-Mono / VINS-Fusion

香港科技大学VINS系列是优秀的视觉惯性SLAM方案：
- **紧耦合VIO**：IMU与视觉深度融合
- **IMU预积分**：避免重复积分
- **在线外参标定**：自动估计相机-IMU外参
- **重定位模块**：基于DBoW2的回环检测

#### 3.2 RTAB-Map

RTAB-Map（Real-Time Appearance-Based Mapping）是通用的RGB-D和激光SLAM框架：
- 支持多种传感器组合
- 基于词袋的回环检测
- 长期和短期内存管理
- 丰富的地图输出格式

## 四、SLAM在自动驾驶中的应用

### 1. 应用场景

SLAM技术在自动驾驶中有广泛的应用：

**（1）高精度地图制作**
使用SLAM技术采集和建图，制作自动驾驶所需的高精度地图（HD Map）。这是目前SLAM在自动驾驶中最成熟的应用之一。

**（2）自主泊车**
在停车场等GNSS拒止环境中，SLAM提供厘米级定位，实现自主代客泊车（AVP）。

**（3）最后一公里配送**
低速物流车在园区、小区等场景使用SLAM进行导航。

**（4）多传感器融合定位**
SLAM作为重要组成部分，与GNSS、IMU等融合，提供全场景定位能力。

### 2. 挑战与解决方案

**（1）动态物体处理**
自动驾驶场景中充满动态物体（车辆、行人），会干扰SLAM的静态环境假设。

*解决方案*：
- 语义SLAM：利用深度学习识别和分割动态物体
- 动态点剔除：基于RANSAC或深度学习剔除动态特征
- 多目标跟踪：同时估计自车和动态物体运动

**（2）大规模环境**
城市级自动驾驶需要处理数十公里的轨迹和海量地图数据。

*解决方案*：
- 滑动窗口优化：只优化最近的若干关键帧
- 地图分层管理：激活地图、休眠地图分级管理
- 分布式/云端SLAM：利用云计算能力

**（3）极端天气条件**
雨雪雾等天气会影响摄像头和激光雷达的性能。

*解决方案*：
- 多传感器冗余：视觉、激光、雷达互为备份
- 自适应特征选择：根据天气条件选择最优特征
- 地图先验辅助：利用先验地图减少实时计算压力

### 3. 发展趋势

**（1）深度学习与SLAM融合**
- 端到端学习：直接学习位姿估计
- 学习特征描述子：如SuperPoint、D2-Net
- 语义SLAM：结合语义信息提高鲁棒性

**（2）多传感器融合**
- 视觉+激光+IMU的紧耦合方案
- 4D成像雷达的加入
- 车路协同（V2X）辅助定位

**（3）终身SLAM（Lifelong SLAM）**
- 长期运行中的地图更新和维护
- 季节变化、环境变化的适应性
- 众包建图与地图更新

## 五、小结

SLAM是自动驾驶定位技术的核心基础。本章介绍了：

1. **SLAM基本概念**：定位与建图联合估计的概率问题
2. **视觉SLAM vs 激光SLAM**：两种主流技术路线的原理与对比
3. **经典算法**：ORB-SLAM系列（视觉）、LOAM/LIO-SAM系列（激光）
4. **应用场景**：从高精度地图制作到实时定位的广泛应用

理解SLAM原理是掌握自动驾驶定位技术的重要一步。后续章节将进一步介绍多传感器融合、高精地图等进阶内容。

---

## 参考文献

[1] Cadena C, Carlone L, Carrillo H, et al. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age[J]. IEEE Transactions on robotics, 2016, 32(6): 1309-1332.

[2] Mur-Artal R, Montiel J M M, Tardós J D. ORB-SLAM: a versatile and accurate monocular SLAM system[J]. IEEE transactions on robotics, 2015, 31(5): 1147-1163.

[3] Zhang J, Singh S. LOAM: Lidar Odometry and Mapping in Real-time[C]//Robotics: Science and Systems. 2014, 2(9): 1-9.

[4] Shan T, Englot B. LeGO-LOAM: Lightweight and ground-optimized lidar odometry and mapping on variable terrain[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 4758-4765.

[5] Shan T, Englot B, Meyers D, et al. LIO-SAM: Tightly-coupled lidar inertial odometry via smoothing and mapping[C]//2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020: 5135-5142.

[6] Qin T, Li P, Shen S. Vins-mono: A robust and versatile monocular visual-inertial state estimator[J]. IEEE Transactions on Robotics, 2018, 34(4): 1004-1020.
